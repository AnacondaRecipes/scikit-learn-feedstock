From 5932172e01bb37595bdba7ad2671f606c5d47975 Mon Sep 17 00:00:00 2001
From: Joel Nothman <joel.nothman@gmail.com>
Date: Tue, 6 Mar 2018 01:05:08 +1100
Subject: [PATCH] FIX n_iter_ should be less than max_iter in when using lbfgs
 solver(#10723)

---
 sklearn/linear_model/huber.py            | 8 ++++++--
 sklearn/linear_model/logistic.py         | 9 ++++++++-
 sklearn/linear_model/tests/test_huber.py | 7 +++++++
 3 files changed, 21 insertions(+), 3 deletions(-)

diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py
index e17dc1e616..3474b849f2 100644
--- a/sklearn/linear_model/huber.py
+++ b/sklearn/linear_model/huber.py
@@ -181,7 +181,11 @@ class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):
 
     n_iter_ : int
         Number of iterations that fmin_l_bfgs_b has run for.
-        Not available if SciPy version is 0.9 and below.
+
+        .. versionchanged:: 0.20
+
+            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
+            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.
 
     outliers_ : array, shape (n_samples,)
         A boolean mask which is set to True where the samples are identified
@@ -272,7 +276,7 @@ class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):
             raise ValueError("HuberRegressor convergence failed:"
                              " l-BFGS-b solver terminated with %s"
                              % dict_['task'].decode('ascii'))
-        self.n_iter_ = dict_.get('nit', None)
+        self.n_iter_ = min(dict_['nit'], self.max_iter)
         self.scale_ = parameters[-1]
         if self.fit_intercept:
             self.intercept_ = parameters[-2]
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index 8646c9a97c..c72a7d951b 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -718,7 +718,9 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
                 warnings.warn("lbfgs failed to converge. Increase the number "
                               "of iterations.")
             try:
-                n_iter_i = info['nit'] - 1
+                # In scipy <= 1.0.0, nit may exceed maxiter.
+                # See https://github.com/scipy/scipy/issues/7854.
+                n_iter_i = min(info['nit'], max_iter)
             except:
                 n_iter_i = info['funcalls'] - 1
         elif solver == 'newton-cg':
@@ -1115,6 +1117,11 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,
         it returns only 1 element. For liblinear solver, only the maximum
         number of iteration across all classes is given.
 
+        .. versionchanged:: 0.20
+
+            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed
+            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.
+
     See also
     --------
     SGDClassifier : incrementally trained logistic regression (when given
diff --git a/sklearn/linear_model/tests/test_huber.py b/sklearn/linear_model/tests/test_huber.py
index 08f4fdf281..ca1092fcd4 100644
--- a/sklearn/linear_model/tests/test_huber.py
+++ b/sklearn/linear_model/tests/test_huber.py
@@ -42,6 +42,13 @@ def test_huber_equals_lr_for_high_epsilon():
     assert_almost_equal(huber.intercept_, lr.intercept_, 2)
 
 
+def test_huber_max_iter():
+    X, y = make_regression_with_outliers()
+    huber = HuberRegressor(max_iter=1)
+    huber.fit(X, y)
+    assert huber.n_iter_ == huber.max_iter
+
+
 def test_huber_gradient():
     # Test that the gradient calculated by _huber_loss_and_gradient is correct
     rng = np.random.RandomState(1)
-- 
2.17.1

